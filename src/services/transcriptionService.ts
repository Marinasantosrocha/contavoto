// API de transcri√ß√£o usando Web Speech API como fallback
export class TranscriptionService {
  private recognition: any = null;

  constructor() {
    // Verificar se o navegador suporta Web Speech API
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition;
      this.recognition = new SpeechRecognition();
      
      // Configura√ß√µes otimizadas para offline
      this.recognition.continuous = false;
      this.recognition.interimResults = false;
      this.recognition.lang = 'pt-BR';
      this.recognition.maxAlternatives = 1;
      
      // Configura√ß√µes para melhor funcionamento offline
      // Removido grammars que estava causando erro
    }
  }

  // Transcri√ß√£o em tempo real usando Web Speech API (funciona offline)
  async transcribeRealtime(): Promise<string> {
    return new Promise((resolve, reject) => {
      if (!this.recognition) {
        reject(new Error('Speech recognition n√£o suportado neste navegador'));
        return;
      }

      let finalTranscript = '';
      let hasResult = false;

      // Timeout para evitar travamento
      const timeout = setTimeout(() => {
        if (!hasResult) {
          this.recognition.stop();
          reject(new Error('Timeout na transcri√ß√£o. Tente novamente.'));
        }
      }, 15000); // 15 segundos

      this.recognition.onresult = (event: any) => {
        hasResult = true;
        clearTimeout(timeout);
        
        if (event.results && event.results.length > 0) {
          const transcript = event.results[0][0].transcript;
          finalTranscript = transcript;
        }
      };

      this.recognition.onend = () => {
        clearTimeout(timeout);
        if (finalTranscript) {
          resolve(finalTranscript);
        } else if (hasResult) {
          resolve('N√£o foi poss√≠vel entender a fala. Tente falar mais claramente.');
        } else {
          reject(new Error('Nenhuma fala detectada. Verifique se o microfone est√° funcionando.'));
        }
      };

      this.recognition.onerror = (event: any) => {
        clearTimeout(timeout);
        
        let errorMessage = 'Erro na transcri√ß√£o';
        switch (event.error) {
          case 'no-speech':
            errorMessage = 'Nenhuma fala detectada. Tente falar mais alto.';
            break;
          case 'audio-capture':
            errorMessage = 'Erro no microfone. Verifique se est√° funcionando.';
            break;
          case 'not-allowed':
            errorMessage = 'Permiss√£o de microfone negada. Permita o acesso ao microfone.';
            break;
          case 'network':
            errorMessage = 'Erro de rede (mas funciona offline). Tente novamente.';
            break;
          case 'service-not-allowed':
            errorMessage = 'Servi√ßo de reconhecimento n√£o permitido.';
            break;
          default:
            errorMessage = `Erro na transcri√ß√£o: ${event.error}`;
        }
        
        reject(new Error(errorMessage));
      };

      this.recognition.onstart = () => {
        console.log('üé§ Transcri√ß√£o iniciada (funcionando offline)...');
      };

      try {
        this.recognition.start();
      } catch (error) {
        clearTimeout(timeout);
        reject(new Error('Erro ao iniciar reconhecimento de fala.'));
      }
    });
  }

  // Transcri√ß√£o de arquivo de √°udio - redireciona para transcri√ß√£o em tempo real
  async transcribeAudioFile(audioBlob: Blob): Promise<string> {
    // A Web Speech API n√£o funciona bem com arquivos reproduzidos
    // Redirecionar para transcri√ß√£o em tempo real
    throw new Error('Use transcri√ß√£o em tempo real para melhor precis√£o');
  }

  // Verificar se o navegador suporta transcri√ß√£o
  isSupported(): boolean {
    return this.recognition !== null;
  }

  // Parar transcri√ß√£o em andamento
  stop(): void {
    if (this.recognition) {
      this.recognition.stop();
    }
  }
}

// Hook para usar o servi√ßo de transcri√ß√£o
export function useTranscription() {
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const transcriptionService = useRef(new TranscriptionService());

  const transcribeRealtime = useCallback(async (): Promise<string> => {
    setIsTranscribing(true);
    setError(null);

    try {
      const result = await transcriptionService.current.transcribeRealtime();
      return result;
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Erro na transcri√ß√£o';
      setError(errorMessage);
      throw err;
    } finally {
      setIsTranscribing(false);
    }
  }, []);

  const transcribeAudioFile = useCallback(async (audioBlob: Blob): Promise<string> => {
    setIsTranscribing(true);
    setError(null);

    try {
      const result = await transcriptionService.current.transcribeAudioFile(audioBlob);
      return result;
    } catch (err) {
      const errorMessage = err instanceof Error ? err.message : 'Erro na transcri√ß√£o';
      setError(errorMessage);
      throw err;
    } finally {
      setIsTranscribing(false);
    }
  }, []);

  const stopTranscription = useCallback(() => {
    transcriptionService.current.stop();
    setIsTranscribing(false);
  }, []);

  return {
    isTranscribing,
    error,
    transcribeRealtime,
    transcribeAudioFile,
    stopTranscription,
    isSupported: transcriptionService.current.isSupported(),
  };
}
